# -*- coding: utf-8 -*-
"""BERT_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e_IyRAOqwP5_xCFx0Scl8aItj2W0ox74

We are going to do a hugging face implementation of BERT Model

# LOAD THE MODEL
"""

!pip install --upgrade pytorch-pretrained-bert #Install the hugging face library

"""# Load the Bert tokenizer"""

# Import torch and Bert models
import torch 
from pytorch_pretrained_bert import BertTokenizer

#Load the pretrained model tokenizer (vocabulary)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

"""# VOCAB DUMP"""

with open('vocabulary.txt','w') as f:
  #for each token 
  for token in tokenizer.vocab.keys():

    #write the token to the vocabulary file
    f.write(token + '\n')

"""# VOCAB INSIGHTS
1.   There are a total of 30,523 tokens including a ' ' (space) 
2.   Rows 1 - 999 contain [unused] tokens with 5 that are filled 
3.   [PAD]. [UNK], [CLS] ,[SEP] and [MASK]
4.    Rows 1000 to 1996 contains single characters 
5.   **[NOTICE]: ** Hindi Varnas and characters are not all there
6.   The first word starts from 1997 row 
7.   From there the words are sorted based on frequency 
8.   '##' words also crop up in frequency.
"""

# Find single words and single hash words
single_character_array = []
single_character_hash_array = []

for token in tokenizer.vocab.keys():
  if len(token) == 1:
    single_character_array.append(token)

  elif len(token) == 3 and token[0:2] == '##':
    single_character_hash_array.append(token)

print('Single character array' , str(single_character_array) + '\n')
print('Length of Single character array: ', str(len(single_character_array)) + '\n')
print('Single character array' , str(single_character_hash_array) + '\n')
print('Length of Single character hash array: ', str(len(single_character_hash_array)) + '\n')

"""# MISSING-MISSPELT WORDS"""

print('misspeled' in tokenizer.vocab.keys())

print('अ' in tokenizer.vocab.keys())
print('अर्थ' in tokenizer.vocab.keys()) 
print('इ' in tokenizer.vocab.keys())
print('ई' in tokenizer.vocab.keys())
print('ऊ' in tokenizer.vocab.keys())
print('ऊ' in tokenizer.vocab.keys())
print('ऐ' in tokenizer.vocab.keys())
print('घ' in tokenizer.vocab.keys())
print('छ' in tokenizer.vocab.keys())
print('झ' in tokenizer.vocab.keys())
print('Conclusion \n: There are a number of single characters of Hindi \n that are not considered and this can help improve the model of BERT')

"""# Names"""

!pip install wget

#Get te file from internet
import wget

print('Downloading names')

url = 'http://www.gutenberg.org/files/3201/files/NAMES.TXT' 
wget.download(url,'names.txt')

#Find what names and how many names exist in the BERT

count_names = 0
with open('names.txt','rb') as f:
  names = f.readlines()

for name in names:
  try:
    name_decoded = name.rstrip().lower().decode('utf-8')
    if name_decoded in tokenizer.vocab.keys():
      count_names +=1
  except:
    continue
print('There are ',count_names, ' names in the BERT vocabulary ')

"""# NUMBERS"""

#Find what numbers and how many numbers exist in the BERT

count_numbers = 0
for token in tokenizer.vocab.keys():
  if token.isdigit():
    count_numbers +=1
print('There are ',count_numbers, ' numbers in the BERT vocabulary ')